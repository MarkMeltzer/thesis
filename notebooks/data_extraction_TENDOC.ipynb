{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2 as ppdf2\n",
    "import PyPDF4 as ppdf4\n",
    "import pdfrw\n",
    "import pdfminer.high_level\n",
    "import requests\n",
    "from pprint import pprint\n",
    "from bs4 import BeautifulSoup\n",
    "from zipfile import ZipFile\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import random\n",
    "from pdfminer.layout import LAParams\n",
    "from io import StringIO\n",
    "import shutil\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "DATA_PATH = \"../data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128  cftIds found!\n"
     ]
    }
   ],
   "source": [
    "# scrape the etendering search result website for tender ids\n",
    "search_results_url = \"https://etendering.ted.europa.eu/cft/cft-search.html?_caList=1&_procedureTypeForthcoming=1&_procedureTypeOngoing=1&caList=67&closingDateFrom=&closingDateTo=&confirm=Search&procedureTypeForthcoming=&procedureTypeOngoing=&startDateFrom=&startDateTo=&status=&text=&maxResults=250\"\n",
    "res = requests.get(search_results_url)\n",
    "soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "table = soup.find(id=\"row\")\n",
    "tbody = table.find(\"tbody\")\n",
    "rows = tbody.find_all(\"tr\")\n",
    "\n",
    "cftIds = []\n",
    "for row in rows:\n",
    "    cftId = row.find_all(\"td\")[1].find(\"a\")[\"href\"].split(\"cftId\")[-1][1:]\n",
    "    cftIds.append(cftId)\n",
    "print(len(cftIds), \" cftIds found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d1f2ddbc854fb1aacb2d187e256da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127  zips downloaded!\n"
     ]
    }
   ],
   "source": [
    "# get all the zip files containing tender documents\n",
    "base_zip_url = \"https://etendering.ted.europa.eu/document/archive-download.html?cftId=[CFTID]&lngIso=en\"\n",
    "\n",
    "for i, cftId in tqdm(enumerate(cftIds), total=len(cftIds)):\n",
    "    url = base_zip_url.replace(\"[CFTID]\", cftId)\n",
    "    \n",
    "    with open(DATA_PATH + \"/ZIP/\" + cftId + \".zip\", \"wb\") as f:\n",
    "        f.write(requests.get(url).content)\n",
    "        \n",
    "print(i, \" zips downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_annex_1(name):\n",
    "    \"\"\"\n",
    "    Determines whether filename contains any variation of Annex 1\n",
    "    \"\"\"\n",
    "    \n",
    "    for s in [\"Annex I\", \"Annex 01\", \"Annex 1\", \"ANNEX I\"]:\n",
    "        res = name.find(s)\n",
    "\n",
    "        if res != -1:\n",
    "            next_char = name[res + len(s)]\n",
    "            if next_char in [' ', '_', '-'] and \"Appendix\" not in name:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def contains_annex_2(name):\n",
    "    \"\"\"\n",
    "    Determines whether filename contains any variation of Annex 2\n",
    "    \"\"\"\n",
    "    \n",
    "    for s in [\"Annex II\", \"Annex 02\", \"Annex 2\", \"ANNEX II\"]:\n",
    "        res = name.find(s)\n",
    "\n",
    "        if res != -1:\n",
    "            next_char = name[res + len(s)]\n",
    "            if next_char in [' ', '_', '-'] and \"Appendix\" not in name:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# extract the relevant documents\n",
    "counter = 0\n",
    "for i, cftId in enumerate(cftIds):\n",
    "    with ZipFile(DATA_PATH + \"/ZIP/\" + cftId + \".zip\", \"r\") as f:\n",
    "        namelist = f.namelist()\n",
    "        \n",
    "        found_names_A1 = []\n",
    "        found_names_A2 = []\n",
    "        for name in namelist:\n",
    "            if \"Replaced versions\" in name:\n",
    "                # subfolder containing oder versions, ignore\n",
    "                continue\n",
    "            \n",
    "            if contains_annex_2(name) and name[-3:] == \"pdf\":\n",
    "                found_names_A2.append(name)\n",
    "            elif contains_annex_1(name) and name[-3:] == \"pdf\":\n",
    "                found_names_A1.append(name)\n",
    "                \n",
    "        if len(found_names_A1 + found_names_A2) > 0:\n",
    "            # if there are multiple docs, choose whichever was found first\n",
    "            name = (found_names_A2 + found_names_A1)[0]\n",
    "            \n",
    "            # extract and rename relevant pdfs\n",
    "            f.extract(name, path=DATA_PATH + \"/PDF/\")\n",
    "            os.rename(DATA_PATH + \"/PDF/\" + name, DATA_PATH + \"/PDF/\" + cftId + \" - \" + name)\n",
    "            counter += 1\n",
    "        \n",
    "        if len(found_names_A1 + found_names_A2) == 0: print(f\"No Annex 2 or Annex 1 found in {cftId}.zip!\")\n",
    "        if len(found_names_A2) > 1: print(f\"Multiple annex 2 found in {cftId}.zip!\")\n",
    "        if len(found_names_A1) > 1: print(f\"Multiple annex 1 found in {cftId}.zip!\")\n",
    "print(f\"{counter} pdf's extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting full text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip because its not the right document\n",
    "skip = [\"2800 - EN-Annex II_declaration on honour_OP_529_2017_RS.pdf\", \"3095 - EN-Annex II MarInt Standard Reply Form_1187.pdf\",\n",
    "        \"7317 - EN-Annex II - Declaration on Honour - FSA I.pdf\",\n",
    "        \"7952 - EN-Annex II - Personnel requirements.pdf\"]\n",
    "# skip because they are scanned documents and can't easily be processed\n",
    "skip += [\"1811 - EN-Annex 1 - Tender Specifications.pdf\", \"2189 - EN-Annex II ToR part B and part A.pdf\"]\n",
    "\n",
    "# save the dates of all documents\n",
    "date_dict = {}\n",
    "for row in rows:\n",
    "    cftId = row.find_all(\"td\")[1].find(\"a\")[\"href\"].split(\"cftId\")[-1][1:]\n",
    "    date_dict[cftId] = row.find(id=\"cft.search.start_date\").text.replace(\"/\", \"-\")\n",
    "\n",
    "# create dataframe\n",
    "data = pd.DataFrame(columns=[\n",
    "    \"filename\",\n",
    "    \"date\",\n",
    "    \"full_text\",\n",
    "    \"selected_sections\"\n",
    "])\n",
    "\n",
    "# fill dataset with full text and date\n",
    "for pdf_filename in tqdm(os.listdir(DATA_PATH + \"/PDF\")):\n",
    "    if pdf_filename in skip:\n",
    "        continue\n",
    "        \n",
    "    # load the full text of a given tender document\n",
    "    pdf_path = DATA_PATH + '/PDF/' + pdf_filename\n",
    "    full_text = pdfminer.high_level.extract_text(pdf_path)\n",
    "    cftId = pdf_filename.split(\" -\")[0]\n",
    "    \n",
    "    # add document to dataset\n",
    "    data = data.append({\n",
    "        \"filename\" : pdf_filename,\n",
    "        \"date\" : date_dict[cftId],\n",
    "        \"full_text\" : full_text,\n",
    "        \"selected_sections\" : None\n",
    "    }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle(DATA_PATH + \"/processed/dataset_TENDOC.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(DATA_PATH + \"/processed/dataset_TENDOC.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting contents by section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_from_section(section, full_text, section_matches):\n",
    "    \"\"\"\n",
    "    Returns the contents of a section given the section, full text and section indices.\n",
    "    \"\"\"\n",
    "    \n",
    "    for i, (s, m) in enumerate(section_matches):\n",
    "        if s == section:\n",
    "            # starting index\n",
    "            current_index = m\n",
    "            \n",
    "            # ending index\n",
    "            if i != len(section_matches) - 1:\n",
    "                next_index = section_matches[i + 1][1]\n",
    "            else:\n",
    "                # the section to find is the last section\n",
    "                next_index = len(full_text)\n",
    "            return full_text[current_index:next_index]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dictionary containing the section names of the documents\n",
    "with open(DATA_PATH + \"/JSON/section_names.json\", \"r\", encoding='utf-8') as f:\n",
    "    section_names = json.load(f)\n",
    "\n",
    "# save the contents of all sections of all pdfs to a dict\n",
    "pdf_contents = {}\n",
    "n = 0\n",
    "for pdf_filename in tqdm(os.listdir(DATA_PATH + \"/PDF\")):\n",
    "    if pdf_filename in skip:\n",
    "        continue\n",
    "        \n",
    "    # get the full text of a document\n",
    "    full_text = data[data[\"filename\"] == pdf_filename][\"full_text\"].iloc[0]\n",
    "\n",
    "    # find the indices of the section headers in the full text\n",
    "    section_matches = []\n",
    "    section_strings = section_names[\"files\"][pdf_filename][\"sections\"]\n",
    "    for section in list(section_strings):\n",
    "        # find index of section header\n",
    "        if section_strings[section] is None:\n",
    "            # if section header can be found using regex, use regex\n",
    "            regex = \"(\\d|I|V)\\.?\\s+?[WORD]\\s+?\\n\".replace(\"[WORD]\", re.escape(section))\n",
    "            matches = list(re.finditer(regex, full_text))\n",
    "            \n",
    "            if len(matches) == 0:\n",
    "                n += 1\n",
    "                print(n, \"- Cannot find any matches for: \", section, \" in \", pdf_filename, \"using regex\")\n",
    "                continue\n",
    "\n",
    "            if len(matches) > 1:\n",
    "                # assumption: if section title is found multiple times,\n",
    "                # the first one is in the table of contents\n",
    "                match = matches[1]\n",
    "            else:\n",
    "                match = matches[0]\n",
    "                \n",
    "            match_i = match.span()[0]\n",
    "        else:\n",
    "            # use fallback if regex doesn't work\n",
    "            search_string = section_strings[section]\n",
    "            match_i = full_text.find(search_string)\n",
    "            \n",
    "            if match_i == -1:\n",
    "                n += 1\n",
    "                print(n, \"- Cannot find any matches for: \", search_string, \" in \", pdf_filename, \"using str.find\")\n",
    "                continue\n",
    "\n",
    "        section_matches.append((section, match_i))\n",
    "    \n",
    "    # find the content of all sections\n",
    "    sections_content = {}\n",
    "    for section in section_names[\"files\"][pdf_filename][\"sections\"]:\n",
    "        content = get_content_from_section(section, full_text, section_matches)\n",
    "        sections_content[section] = content\n",
    "        \n",
    "    # save the section contents per file\n",
    "    pdf_contents[pdf_filename] = sections_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2958 - EN-Annex II_841_Terms of reference.pdf'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(section_names[\"files\"].keys())[34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f12b14527df4c5f808a6306d1751d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the dictionary containing the section names of the documents\n",
    "with open(DATA_PATH + \"/JSON/section_names.json\", \"r\", encoding='utf-8') as f:\n",
    "    section_names = json.load(f)\n",
    "\n",
    "selected_sections_series = []\n",
    "for pdf_filename in tqdm(os.listdir(DATA_PATH + \"/PDF\")):\n",
    "    if pdf_filename in skip:\n",
    "        continue\n",
    "    \n",
    "    selected_sections = \"\"\n",
    "    for section in section_names[\"files\"][pdf_filename][\"selected_sections\"]:\n",
    "        selected_sections += pdf_contents[pdf_filename][section]\n",
    "        \n",
    "    selected_sections_series.append(selected_sections)\n",
    "\n",
    "data[\"selected_sections\"] = selected_sections_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle(DATA_PATH + \"/processed/dataset_TENDOC.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
